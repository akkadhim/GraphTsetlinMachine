{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "usage: ipykernel_launcher.py [-h] [--epochs EPOCHS]\n",
      "                             [--number-of-clauses NUMBER_OF_CLAUSES] [--T T]\n",
      "                             [--s S]\n",
      "                             [--number-of-state-bits NUMBER_OF_STATE_BITS]\n",
      "                             [--depth DEPTH]\n",
      "                             [--hypervector-size HYPERVECTOR_SIZE]\n",
      "                             [--hypervector-bits HYPERVECTOR_BITS]\n",
      "                             [--message-size MESSAGE_SIZE]\n",
      "                             [--message-bits MESSAGE_BITS] [--double-hashing]\n",
      "                             [--noise NOISE]\n",
      "                             [--max-included-literals MAX_INCLUDED_LITERALS]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/root/.local/share/jupyter/runtime/kernel-v306f6e67794e909fd94dbef768cafee2e613728cc.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from GraphTsetlinMachine.graphs import Graphs\n",
    "from GraphTsetlinMachine.tm import MultiClassGraphTsetlinMachine\n",
    "from time import time\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import kagglehub\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def default_args(**kwargs):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--epochs\", default=250, type=int)\n",
    "    parser.add_argument(\"--number-of-clauses\", default=10000, type=int)\n",
    "    parser.add_argument(\"--T\", default=10000, type=int)\n",
    "    parser.add_argument(\"--s\", default=10.0, type=float)\n",
    "    parser.add_argument(\"--number-of-state-bits\", default=8, type=int)\n",
    "    parser.add_argument(\"--depth\", default=1, type=int)\n",
    "    parser.add_argument(\"--hypervector-size\", default=4096, type=int)\n",
    "    parser.add_argument(\"--hypervector-bits\", default=256, type=int)\n",
    "    parser.add_argument(\"--message-size\", default=4096, type=int)\n",
    "    parser.add_argument(\"--message-bits\", default=256, type=int)\n",
    "    parser.add_argument('--double-hashing', dest='double_hashing', default=False, action='store_true')\n",
    "    parser.add_argument(\"--noise\", default=0.01, type=float)\n",
    "    parser.add_argument(\"--max-included-literals\", default=10, type=int)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    for key, value in kwargs.items():\n",
    "        if key in args.__dict__:\n",
    "            setattr(args, key, value)\n",
    "    return args\n",
    "\n",
    "args = default_args()\n",
    "\n",
    "############################# real dataset ########################\n",
    "\n",
    "print(\"Creating training data\")\n",
    "path = kagglehub.dataset_download(\"karkavelrajaj/amazon-sales-dataset\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "data_file = path + \"/amazon.csv\" \n",
    "org_data = pd.read_csv(data_file)\n",
    "# print(\"Data preview:\", data.head())\n",
    "org_data = org_data[['product_id', 'category', 'user_id', 'rating']]\n",
    "#################################### expanded \n",
    "org_data['rating'] = pd.to_numeric(org_data['rating'], errors='coerce')  # Coerce invalid values to NaN\n",
    "org_data.dropna(subset=['rating'], inplace=True)  # Drop rows with NaN ratings\n",
    "org_data['rating'] = org_data['rating'].astype(int)\n",
    "# Expand the dataset 10 times\n",
    "data = pd.concat([org_data] * 10, ignore_index=True)\n",
    "\n",
    "# Shuffle the expanded dataset\n",
    "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Add noise\n",
    "# Define the noise ratio\n",
    "noise_ratio = 0.1  # 10% noise\n",
    "\n",
    "# Select rows to apply noise\n",
    "num_noisy_rows = int(noise_ratio * len(data))\n",
    "noisy_indices = np.random.choice(data.index, size=num_noisy_rows, replace=False)\n",
    "\n",
    "# Add noise to ratings\n",
    "data.loc[noisy_indices, 'rating'] = np.random.choice(range(1, 6), size=num_noisy_rows)\n",
    "\n",
    "# Add noise to categories\n",
    "unique_categories = data['category'].unique()\n",
    "data.loc[noisy_indices, 'category'] = np.random.choice(unique_categories, size=num_noisy_rows)\n",
    "\n",
    "# Print a preview of the noisy and expanded dataset\n",
    "print(\"Original data shape:\", org_data.shape)\n",
    "print(\"Expanded data shape:\", data.shape)\n",
    "print(\"Data preview:\\n\", data.head())\n",
    "\n",
    "print(data.head())\n",
    " \n",
    "le_user = LabelEncoder()\n",
    "le_item = LabelEncoder()\n",
    "le_category = LabelEncoder()\n",
    "le_rating = LabelEncoder() \n",
    "\n",
    "data['user_id'] = le_user.fit_transform(data['user_id'])\n",
    "data['product_id'] = le_item.fit_transform(data['product_id'])\n",
    "data['category'] = le_category.fit_transform(data['category'])\n",
    "data['rating'] = le_rating.fit_transform(data['rating'])\n",
    "\n",
    "x = data[['user_id', 'product_id', 'category']].values  \n",
    "y = data['rating'].values \n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", Y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", Y_test.shape)\n",
    "\n",
    "users = data['user_id'].unique()\n",
    "items = data['product_id'].unique()\n",
    "categories = data['category'].unique()\n",
    "\n",
    "# Initialize Graphs with symbols for GTM\n",
    "number_of_nodes = 3\n",
    "symbols = []\n",
    "symbols = [\"U_\" + str(u) for u in users] + [\"I_\" + str(i) for i in items] + [\"C_\" + str(c) for c in categories] \n",
    "print(len(symbols))\n",
    "# Train data\n",
    "graphs_train = Graphs(\n",
    "    X_train.shape[0],\n",
    "    symbols=symbols,\n",
    "    hypervector_size=args.hypervector_size,\n",
    "    hypervector_bits=args.hypervector_bits,\n",
    "    double_hashing = args.double_hashing\n",
    ")\n",
    "for graph_id in range(X_train.shape[0]):\n",
    "    graphs_train.set_number_of_graph_nodes(graph_id, number_of_nodes)\n",
    "graphs_train.prepare_node_configuration()\n",
    "for graph_id in range(X_train.shape[0]):\n",
    "    for node_id in range(graphs_train.number_of_graph_nodes[graph_id]):\n",
    "        number_of_edges = 2 if node_id > 0 and node_id < graphs_train.number_of_graph_nodes[graph_id]-1 else 1\n",
    "        if node_id == 0:\n",
    "            graphs_train.add_graph_node(graph_id, \"User\", number_of_edges)\n",
    "        elif node_id == 1:\n",
    "            graphs_train.add_graph_node(graph_id, \"Item\", number_of_edges)\n",
    "        else:\n",
    "            graphs_train.add_graph_node(graph_id, \"Category\", number_of_edges)\n",
    "graphs_train.prepare_edge_configuration()\n",
    "for graph_id in range(X_train.shape[0]):\n",
    "    for node_id in range(graphs_train.number_of_graph_nodes[graph_id]):\n",
    "        if node_id == 0:\n",
    "            graphs_train.add_graph_node_edge(graph_id, \"User\", \"Item\", \"UserItem\")\n",
    "            \n",
    "        if node_id == 1:\n",
    "            graphs_train.add_graph_node_edge(graph_id, \"Item\", \"Category\", \"ItemCategory\")\n",
    "            graphs_train.add_graph_node_edge(graph_id, \"Item\", \"User\", \"ItemUser\")\n",
    "            \n",
    "        if node_id == 2:\n",
    "            graphs_train.add_graph_node_edge(graph_id, \"Category\", \"Item\", \"CatrgoryItem\")\n",
    "\n",
    "    graphs_train.add_graph_node_property(graph_id, \"User\", \"U_\" + str(X_train[graph_id][0]))\n",
    "    graphs_train.add_graph_node_property(graph_id, \"Item\", \"I_\" + str(X_train[graph_id][1]))\n",
    "    graphs_train.add_graph_node_property(graph_id, \"Category\", \"C_\" + str(X_train[graph_id][2]))\n",
    "graphs_train.encode()\n",
    "print(\"Training data produced\")\n",
    "\n",
    "# Test data\n",
    "graphs_test = Graphs(X_test.shape[0], init_with=graphs_train)\n",
    "for graph_id in range(X_test.shape[0]):\n",
    "    graphs_test.set_number_of_graph_nodes(graph_id, number_of_nodes)\n",
    "graphs_test.prepare_node_configuration()\n",
    "for graph_id in range(X_test.shape[0]):\n",
    "    for node_id in range(graphs_test.number_of_graph_nodes[graph_id]):\n",
    "        number_of_edges = 2 if node_id > 0 and node_id < graphs_test.number_of_graph_nodes[graph_id]-1 else 1\n",
    "        if node_id == 0:\n",
    "            graphs_test.add_graph_node(graph_id, \"User\", number_of_edges)\n",
    "        elif node_id == 1:\n",
    "            graphs_test.add_graph_node(graph_id, \"Item\", number_of_edges)\n",
    "        else:\n",
    "            graphs_test.add_graph_node(graph_id, \"Category\", number_of_edges)\n",
    "graphs_test.prepare_edge_configuration()\n",
    "for graph_id in range(X_test.shape[0]):\n",
    "    for node_id in range(graphs_test.number_of_graph_nodes[graph_id]):\n",
    "        if node_id == 0:\n",
    "            graphs_test.add_graph_node_edge(graph_id, \"User\", \"Item\", \"UserItem\")\n",
    "            \n",
    "        if node_id == 1:\n",
    "            graphs_test.add_graph_node_edge(graph_id, \"Item\", \"Category\", \"ItemCategory\")\n",
    "            graphs_test.add_graph_node_edge(graph_id, \"Item\", \"User\", \"ItemUser\")\n",
    "            \n",
    "        if node_id == 2:\n",
    "            graphs_test.add_graph_node_edge(graph_id, \"Category\", \"Item\", \"CatrgoryItem\")\n",
    "\n",
    "    graphs_test.add_graph_node_property(graph_id, \"User\", \"U_\" + str(X_test[graph_id][0]))\n",
    "    graphs_test.add_graph_node_property(graph_id, \"Item\", \"I_\" + str(X_test[graph_id][1]))\n",
    "    graphs_test.add_graph_node_property(graph_id, \"Category\", \"C_\" + str(X_test[graph_id][2]))\n",
    "graphs_test.encode()\n",
    "print(\"Testing data produced\")\n",
    "\n",
    "tm = MultiClassGraphTsetlinMachine(\n",
    "    args.number_of_clauses,\n",
    "    args.T,\n",
    "    args.s,\n",
    "    number_of_state_bits = args.number_of_state_bits,\n",
    "    depth=args.depth,\n",
    "    message_size=args.message_size,\n",
    "    message_bits=args.message_bits,\n",
    "    max_included_literals=args.max_included_literals,\n",
    "    double_hashing = args.double_hashing\n",
    ")\n",
    "\n",
    "for i in range(args.epochs):\n",
    "    start_training = time()\n",
    "    tm.fit(graphs_train, Y_train, epochs=1, incremental=True)\n",
    "    stop_training = time()\n",
    "\n",
    "    start_testing = time()\n",
    "    result_test = 100*(tm.predict(graphs_test) == Y_test).mean()\n",
    "    stop_testing = time()\n",
    "\n",
    "    result_train = 100*(tm.predict(graphs_train) == Y_train).mean()\n",
    "\n",
    "    print(\"%d %.2f %.2f %.2f %.2f\" % (i, result_train, result_test, stop_training-start_training, stop_testing-start_testing))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
